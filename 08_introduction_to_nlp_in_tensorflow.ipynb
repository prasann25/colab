{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_introduction_to_nlp_in_tensorflow",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasann25/colab/blob/main/08_introduction_to_nlp_in_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8fbclEDHYdu"
      },
      "source": [
        "# Introduction to NLP Fundamentals in TensorFlow\n",
        "\n",
        "NLP has the goal of deriving information out of natural language(could be sequences text or speech).\n",
        "Another common term for NLP problems is sequence to sequence problems(seq2seq)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iotl9tC7k8U7"
      },
      "source": [
        "## Check for GPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FjVZH_blBRM",
        "outputId": "ef571252-fa81-46ac-9cbb-3d113a117fc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-b470b110-2222-4c7d-cb00-cea7e71131d5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJueI-mTlGvR"
      },
      "source": [
        "## Get helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY5LbWNzlPNN",
        "outputId": "0c1164fb-7363-486c-e386-8e061831391f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "# Import series of helper functions for the notebook\n",
        "from helper_functions import unzip_data, create_tensorboard_callback, compare_historys, plot_loss_curves"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-04 00:35:42--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py.1’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-08-04 00:35:43 (112 MB/s) - ‘helper_functions.py.1’ saved [10246/10246]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9d3U2Stlp7z"
      },
      "source": [
        "## Get a text dataset\n",
        "\n",
        "The dataset we're going to be using is Kaggle's introduction to NLP dataset(text samples of Tweets labelled as disaster or not disaster).\n",
        "\n",
        "See the original source here : https://www.kaggle.com/c/nlp-getting-started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jexI6M-Dl9p0",
        "outputId": "7560db86-d451-40b6-dc53-dd95a2a8dc4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-04 00:38:04--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.101.128, 142.250.141.128, 142.251.2.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.101.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip’\n",
            "\n",
            "\rnlp_getting_started   0%[                    ]       0  --.-KB/s               \rnlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2021-08-04 00:38:05 (142 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw2RJ1PvmLVw"
      },
      "source": [
        "# Unzip data\n",
        "unzip_data(\"nlp_getting_started.zip\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV3ckHJJmaPj"
      },
      "source": [
        "## Become one with the data"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}